@inproceedings{iso18004,
  title     = {{ISO/IEC 18004: Information technology -- Automatic identification and data capture techniques -- QR code bar code symbology specification}},
  booktitle = {ISO/IEC 18004:2000},
  author    = {{International Organization for Standardization}},
  year      = {2000},
}

//LLM
@article{PLMsPreTraining,
   title={Pre-trained models for natural language processing: A survey},
   volume={63},
   ISSN={1869-1900},
   url={http://dx.doi.org/10.1007/s11431-020-1647-3},
   DOI={10.1007/s11431-020-1647-3},
   number={10},
   journal={Science China Technological Sciences},
   publisher={Springer Science and Business Media LLC},
   author={Qiu, XiPeng and Sun, TianXiang and Xu, YiGe and Shao, YunFan and Dai, Ning and Huang, XuanJing},
   year={2020},
   month=sep, pages={1872–1897} 

}

@online{krannig2020deep,
  author       = {Krannig, Simon},
  title        = {Deep Learning schafft Serverstabilität und verbessert Netzwerkarchitekturen},
  date         = {2020-03-19},
  howpublished = {Blog-Beitrag, Adacor Hosting GmbH, Bereich „News \& Trends“},
  url          = {https://blog.adacor.com/deep-learning-schafft-serverstabilitaet-und-verbessert-netzwerkarchitekturen_8414.html},
  note         = {Abgerufen am \today},
}


@inproceedings{AttentionIsAllYouNeed,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{SCHMIDHUBER201585,
  title = {Deep learning in neural networks: An overview},
  journal = {Neural Networks},
  volume = {61},
  pages = {85-117},
  year = {2015},
  issn = {0893-6080},
  doi = {https://doi.org/10.1016/j.neunet.2014.09.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608014002135},
  author = {Jürgen Schmidhuber},
  keywords = {Deep learning, Supervised learning, Unsupervised learning, Reinforcement learning, Evolutionary computation},
  abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.}
}

@INPROCEEDINGS{8666928,
  author={Wang, Wei and Gang, Jianxun},
  booktitle={2018 International Conference on Information Systems and Computer Aided Education (ICISCAE)}, 
  title={Application of Convolutional Neural Network in Natural Language Processing}, 
  year={2018},
  volume={},
  number={},
  pages={64-70},
  abstract={Convolutional neural network (Convolutionl Neural Network, CNN) is a multiple-layer neural network method to learn hierarchical characteristic of data. In recent years, CNN has developed rapidly in the design and calculation of natural language processing (NLP). This paper introduces the principles models and applications of CNN in natural language processing tasks and presents some personal insights into the use of CNN methods in NLP task processing.},
  keywords={Task analysis;Natural language processing;Semantics;Neural networks;Context modeling;Feature extraction;Convolution;Natural language processing;Convolution neural network;Applications},
  doi={10.1109/ICISCAE.2018.8666928},
  ISSN={},
  month={July},
}

@article{PLMsPaper,
  title={Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey},
  author={Bonan Min and Hayley Ross and Elior Sulem and Amir Pouran Ben Veyseh and Thien Huu Nguyen and Oscar Sainz and Eneko Agirre and Ilana Heinz and Dan Roth},
  journal={ACM Computing Surveys},
  year={2021},
  volume={56},
  pages={1 - 40},
  url={https://api.semanticscholar.org/CorpusID:240420063}
}

@misc{brown2020languagemodelsfewshotlearners,
  title={Language Models are Few-Shot Learners}, 
  author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  year={2020},
  eprint={2005.14165},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2005.14165}, 
}

@misc{LLMTaxonomyPrompting,
  title={Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey}, 
  author={Chen Ling and Xujiang Zhao and Jiaying Lu and Chengyuan Deng and Can Zheng and Junxiang Wang and Tanmoy Chowdhury and Yun Li and Hejie Cui and Xuchao Zhang and Tianjiao Zhao and Amit Panalkar and Dhagash Mehta and Stefano Pasquali and Wei Cheng and Haoyu Wang and Yanchi Liu and Zhengzhang Chen and Haifeng Chen and Chris White and Quanquan Gu and Jian Pei and Carl Yang and Liang Zhao},
  year={2024},
  eprint={2305.18703},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2305.18703}, 
}

@article{Inference,
  title={LLM Inference Unveiled: Survey and Roofline Model Insights},
  author={Zhihang Yuan and Yuzhang Shang and Yang Zhou and Zhen Dong and Chenhao Xue and Bingzhe Wu and Zhikai Li and Qingyi Gu and Yong Jae Lee and Yan Yan and Beidi Chen and Guangyu Sun and Kurt Keutzer},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.16363},
  url={https://api.semanticscholar.org/CorpusID:268032253}
}

@INPROCEEDINGS{10825265,
  author={Keluskar, Aryan and Bhattacharjee, Amrita and Liu, Huan},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Do LLMs Understand Ambiguity in Text? A Case Study in Open-world Question Answering}, 
  year={2024},
  volume={},
  number={},
  pages={7485-7490},
  keywords={Sentiment analysis;Uncertainty;Large language models;Natural languages;Focusing;Big Data;Feature extraction;Question answering (information retrieval);Data models;Best practices;ambiguity;sensitivity;LLM;large language model;question-answering},
  doi={10.1109/BigData62323.2024.10825265}
}

@book{ExplainableLLM,
  title={A Comprehensive Guide to Explainable AI: From Classical Models to LLMs}, 
  author={Weiche Hsieh and Ziqian Bi and Chuanqi Jiang and Junyu Liu and Benji Peng and Sen Zhang and Xuanhe Pan and Jiawei Xu and Jinlang Wang and Keyu Chen and Pohsun Feng and Yizhu Wen and Xinyuan Song and Tianyang Wang and Ming Liu and Junjie Yang and Ming Li and Bowen Jing and Jintao Ren and Junhao Song and Hong-Ming Tseng and Yichao Zhang and Lawrence K. Q. Yan and Qian Niu and Silin Chen and Yunze Wang and Chia Xin Liang},
  year={2024},
  eprint={2412.00800},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2412.00800}, 
}

@misc{li2024personalllmagentsinsights,
  title={Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security}, 
  author={Yuanchun Li and Hao Wen and Weijun Wang and Xiangyu Li and Yizhen Yuan and Guohong Liu and Jiacheng Liu and Wenxing Xu and Xiang Wang and Yi Sun and Rui Kong and Yile Wang and Hanfei Geng and Jian Luan and Xuefeng Jin and Zilong Ye and Guanjing Xiong and Fan Zhang and Xiang Li and Mengwei Xu and Zhijun Li and Peng Li and Yang Liu and Ya-Qin Zhang and Yunxin Liu},
  year={2024},
  eprint={2401.05459},
  archivePrefix={arXiv},
  primaryClass={cs.HC},
  url={https://arxiv.org/abs/2401.05459}, 
}

//RAG
@misc{ibm2023rag,
  author       = {{IBM Research}},
  title        = {Retrieval-Augmented Generation (RAG): Why adding search to generative AI makes it better},
  year         = {2023},
  url          = {https://research.ibm.com/blog/retrieval-augmented-generation-RAG},
  note         = {Accessed: 2025-05-20}
}
