{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tiktoken\n",
    "import fitz\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qdrant_client() -> QdrantClient:\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    # IP und Port aus Umgebungsvariablen lesen\n",
    "    host = os.getenv(\"QDRANT_SERVER_IP\", \"localhost\")\n",
    "    port = int(os.getenv(\"QDRANT_PORT\", 6333))\n",
    "\n",
    "    try:\n",
    "        client = QdrantClient(host=host, port=port)\n",
    "\n",
    "        # Verbindung testen (z.‚ÄØB. durch Auflisten der Collections)\n",
    "        _ = client.get_collections()\n",
    "\n",
    "        print(f\"Verbindung zu Qdrant unter {host}:{port} erfolgreich!\")\n",
    "        return client\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei der Verbindung zu Qdrant ({host}:{port}): {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_qdrant_alive(client) -> bool:\n",
    "    \"\"\"\n",
    "    Pr√ºft, ob Qdrant erreichbar ist, indem eine einfache Anfrage gestellt wird.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client.get_collections()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Verbindungspr√ºfung fehlgeschlagen: {e}\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_and_chunk(filepath: str, chunk_size: int = 500, overlap: int = 50) -> list[str]:\n",
    "    # Existenz der Datei pr√ºfen\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Die Datei wurde nicht gefunden: {filepath}\")\n",
    "\n",
    "    # PDF einlesen\n",
    "    doc = fitz.open(filepath)\n",
    "    full_text = \"\"\n",
    "    for page in doc:\n",
    "        full_text += page.get_text(\"text\") + \"\\n\"  # \"\\n\" trennt die Seiten\n",
    "    doc.close()\n",
    "\n",
    "    # Chunks erzeugen mit √úberlappung\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(full_text):\n",
    "        end = min(start + chunk_size, len(full_text))\n",
    "        chunks.append(full_text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model: str = \"text-embedding-3-large\") -> list:\n",
    "    \"\"\"\n",
    "    Erstellt ein Embedding f√ºr einen gegebenen Text √ºber die OpenAI API.\n",
    "\n",
    "    Args:\n",
    "        text (str): Der zu embeddene Text\n",
    "        model (str): Das OpenAI-Embedding-Modell (default: text-embedding-3-large)\n",
    "\n",
    "    Returns:\n",
    "        list: Embedding-Vektor als Liste von Floats\n",
    "    \"\"\"\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    if not api_key:\n",
    "        raise ValueError(\"OPENAI_API_KEY wurde nicht in der .env-Datei gefunden.\")\n",
    "\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            input=text,\n",
    "            model=model\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Abrufen des Embeddings: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_chunks(chunks: list[str]) -> list[list[float]]:\n",
    "    return [get_embedding(chunk) for chunk in chunks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
    "\n",
    "def store_embeddings_in_qdrant(client, collection_name: str, chunks: list[str], embeddings: list[list[float]]):\n",
    "    # Collection neu erstellen (l√∂scht alte Inhalte!)\n",
    "    client.recreate_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=len(embeddings[0]), distance=Distance.COSINE)\n",
    "    )\n",
    "\n",
    "    # Punkte erstellen\n",
    "    points = [\n",
    "        PointStruct(id=i, vector=vector, payload={\"text\": chunks[i]})\n",
    "        for i, vector in enumerate(embeddings)\n",
    "    ]\n",
    "\n",
    "    # Hochladen\n",
    "    client.upsert(collection_name=collection_name, points=points)\n",
    "\n",
    "    print(f\"{len(points)} Embeddings in Qdrant gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_similar_chunks(query: str, client: QdrantClient, collection_name: str, top_k: int = 5) -> list[str]:\n",
    "    \"\"\"\n",
    "    F√ºhrt eine semantische Suche in Qdrant basierend auf einer Query durch.\n",
    "\n",
    "    Args:\n",
    "        query (str): Die Nutzerfrage\n",
    "        client (QdrantClient): Verbundener Qdrant-Client\n",
    "        collection_name (str): Name der zu durchsuchenden Collection\n",
    "        top_k (int): Anzahl der zur√ºckgegebenen √§hnlichen Chunks\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Liste der √§hnlichsten Text-Chunks\n",
    "    \"\"\"\n",
    "    query_vector = get_embedding(query)\n",
    "    if not query_vector:\n",
    "        return []\n",
    "\n",
    "    search_result = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=top_k\n",
    "    )\n",
    "\n",
    "    return [hit.payload[\"text\"] for hit in search_result]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_context(query: str, context_chunks: list[str], model: str = \"gpt-4o\") -> str:\n",
    "    \"\"\"\n",
    "    Nutzt OpenAI GPT-4o, um eine Antwort auf eine Frage zu geben ‚Äì basierend auf den gegebenen Kontext-Chunks.\n",
    "\n",
    "    Args:\n",
    "        query (str): Die Benutzerfrage\n",
    "        context_chunks (list[str]): Liste von Texten aus Qdrant\n",
    "        model (str): OpenAI-Modellname (standardm√§√üig GPT-4o)\n",
    "\n",
    "    Returns:\n",
    "        str: Generierte Antwort\n",
    "    \"\"\"\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"OPENAI_API_KEY wurde nicht in der .env-Datei gefunden.\")\n",
    "\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    # Prompt zusammenbauen\n",
    "    context = \"\\n\\n\".join(context_chunks)\n",
    "    prompt = f\"Beantworte die folgende Frage basierend auf dem Kontext:\\n\\nKontext:\\n{context}\\n\\nFrage: {query}\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher Assistent f√ºr wissenschaftliche Fragen.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei der Antwortgenerierung: {e}\")\n",
    "        return \"Fehler bei der Antwortgenerierung.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbindung zu Qdrant unter 152.53.228.53:6333 erfolgreich!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/06/y765zhbd3xg97g0s0prnz96r0000gn/T/ipykernel_80466/1369516826.py:5: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Embeddings in Qdrant gespeichert.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/06/y765zhbd3xg97g0s0prnz96r0000gn/T/ipykernel_80466/2305006633.py:18: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Antwort von GPT-4o:\n",
      "Der Abstract enth√§lt keine spezifischen Informationen √ºber das Datum, an dem er geschrieben wurde. Um das genaue Datum zu bestimmen, w√§re es notwendig, auf zus√§tzliche Informationen wie das Ver√∂ffentlichungsdatum der Arbeit oder das Datum der Einreichung zuzugreifen, die in dem bereitgestellten Kontext nicht enthalten sind.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    client = get_qdrant_client()\n",
    "    is_qdrant_alive(client)\n",
    "\n",
    "    pdf_path = \"/Users/i589466/Desktop/Datenbanken/Datenbanken/Konz_Julian_Abstract_PA2.pdf\"\n",
    "    chunks = load_pdf_and_chunk(pdf_path, chunk_size=500, overlap=50)\n",
    "    embeddings = embed_chunks(chunks)\n",
    "    store_embeddings_in_qdrant(client, \"db_Benny\", chunks, embeddings)\n",
    "\n",
    "    if is_qdrant_alive(client):\n",
    "        query= \"Wann wurde der Abstract geschrieben?\"\n",
    "        retrieved_chunks = retrieve_similar_chunks(query, client, \"db_Benny\", top_k=5)\n",
    "        answer = answer_with_context(query, retrieved_chunks)\n",
    "        print(f\"\\nüß† Antwort von GPT-4o:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
