{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tiktoken\n",
    "import fitz\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qdrant_client() -> QdrantClient:\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    # IP und Port aus Umgebungsvariablen lesen\n",
    "    host = os.getenv(\"QDRANT_SERVER_IP\", \"localhost\")\n",
    "    port = int(os.getenv(\"QDRANT_PORT\", 6333))\n",
    "\n",
    "    try:\n",
    "        client = QdrantClient(host=host, port=port)\n",
    "\n",
    "        # Verbindung testen (z.‚ÄØB. durch Auflisten der Collections)\n",
    "        _ = client.get_collections()\n",
    "\n",
    "        print(f\"Successfully connected to Qdrant at {host}:{port}!\")\n",
    "        return client\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Qdrant at {host}:{port}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_qdrant_alive(client) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if Qdrant is reachable by sending a simple request.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client.get_collections()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Connection check failed: {e}\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_and_chunk(filepath: str, chunk_size: int = 500, overlap: int = 50) -> list[str]:\n",
    "    # Existenz der Datei pr√ºfen\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"The file was not found: {filepath}\")\n",
    "\n",
    "    # PDF einlesen\n",
    "    doc = fitz.open(filepath)\n",
    "    full_text = \"\"\n",
    "    for page in doc:\n",
    "        full_text += page.get_text(\"text\") + \"\\n\"  # \"\\n\" trennt die Seiten\n",
    "    doc.close()\n",
    "\n",
    "    # Chunks erzeugen mit √úberlappung\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(full_text):\n",
    "        end = min(start + chunk_size, len(full_text))\n",
    "        chunks.append(full_text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model: str = \"text-embedding-3-large\") -> list:\n",
    "    \"\"\"\n",
    "    Erstellt ein Embedding f√ºr einen gegebenen Text √ºber die OpenAI API.\n",
    "\n",
    "    Args:\n",
    "        text (str): Der zu embeddene Text\n",
    "        model (str): Das OpenAI-Embedding-Modell (default: text-embedding-3-large)\n",
    "\n",
    "    Returns:\n",
    "        list: Embedding-Vektor als Liste von Floats\n",
    "    \"\"\"\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    if not api_key:\n",
    "        raise ValueError(\"OPENAI_API_KEY wurde nicht in der .env-Datei gefunden.\")\n",
    "\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            input=text,\n",
    "            model=model\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error while retrieving the embedding: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_chunks(chunks: list[str]) -> list[list[float]]:\n",
    "    return [get_embedding(chunk) for chunk in chunks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
    "\n",
    "def store_embeddings_in_qdrant(client, collection_name: str, chunks: list[str], embeddings: list[list[float]]):\n",
    "    # Collection neu erstellen (l√∂scht alte Inhalte!)\n",
    "    client.recreate_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=len(embeddings[0]), distance=Distance.COSINE)\n",
    "    )\n",
    "\n",
    "    # Punkte erstellen\n",
    "    points = [\n",
    "        PointStruct(id=i, vector=vector, payload={\"text\": chunks[i]})\n",
    "        for i, vector in enumerate(embeddings)\n",
    "    ]\n",
    "\n",
    "    # Hochladen\n",
    "    client.upsert(collection_name=collection_name, points=points)\n",
    "\n",
    "    print(f\"{len(points)} Embeddings stored in Qdrant.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_similar_chunks(query: str, client: QdrantClient, collection_name: str, top_k: int = 5) -> list[str]:\n",
    "    \"\"\"\n",
    "    F√ºhrt eine semantische Suche in Qdrant basierend auf einer Query durch.\n",
    "\n",
    "    Args:\n",
    "        query (str): Die Nutzerfrage\n",
    "        client (QdrantClient): Verbundener Qdrant-Client\n",
    "        collection_name (str): Name der zu durchsuchenden Collection\n",
    "        top_k (int): Anzahl der zur√ºckgegebenen √§hnlichen Chunks\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Liste der √§hnlichsten Text-Chunks\n",
    "    \"\"\"\n",
    "    query_vector = get_embedding(query)\n",
    "    if not query_vector:\n",
    "        return []\n",
    "\n",
    "    search_result = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=top_k\n",
    "    )\n",
    "\n",
    "    return [hit.payload[\"text\"] for hit in search_result]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_context(query: str, context_chunks: list[str], model: str = \"gpt-4o\") -> str:\n",
    "    \"\"\"\n",
    "    Nutzt OpenAI GPT-4o, um eine Antwort auf eine Frage zu geben ‚Äì basierend auf den gegebenen Kontext-Chunks.\n",
    "\n",
    "    Args:\n",
    "        query (str): Die Benutzerfrage\n",
    "        context_chunks (list[str]): Liste von Texten aus Qdrant\n",
    "        model (str): OpenAI-Modellname (standardm√§√üig GPT-4o)\n",
    "\n",
    "    Returns:\n",
    "        str: Generierte Antwort\n",
    "    \"\"\"\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"OPENAI_API_KEY was not found in the .env file.\")\n",
    "\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    # Prompt zusammenbauen\n",
    "    context = \"\\n\\n\".join(context_chunks)\n",
    "    prompt = f\"Answer the following question based on the context:\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant for scientific questions.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during answer generation: {e}\")\n",
    "        return \"Error during answer generation.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to Qdrant at 152.53.228.53:6333!\n",
      "üìÑ Loading and processing PDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/06/y765zhbd3xg97g0s0prnz96r0000gn/T/ipykernel_17654/365826436.py:5: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183 Embeddings stored in Qdrant.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/06/y765zhbd3xg97g0s0prnz96r0000gn/T/ipykernel_17654/2305006633.py:18: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Answer from GPT-4o:\n",
      "Large Reasoning Models (LRMs) are specialized variants of Large Language Models (LLMs) that are explicitly designed for reasoning tasks. The key distinctions between LRMs and standard LLMs in terms of architecture and training objectives include:\n",
      "\n",
      "1. **Specialized \"Thinking\" Mechanisms**: LRMs incorporate mechanisms such as long Chain-of-Thought (CoT) processes with self-reflection. These mechanisms are intended to enhance the model's ability to perform complex reasoning tasks by simulating a more human-like thought process.\n",
      "\n",
      "2. **Focus on Reasoning Tasks**: While standard LLMs are generally trained for a broad range of language understanding tasks, LRMs are specifically optimized for reasoning. This involves tailoring their architecture and training to better handle tasks that require logical deduction, problem-solving, and other forms of reasoning.\n",
      "\n",
      "3. **Performance Regimes**: LRMs are designed to perform differently across tasks of varying complexity. They are expected to outperform standard LLMs in medium-complexity tasks due to their enhanced reasoning capabilities, although they may not always excel in low-complexity tasks and can struggle with high-complexity tasks.\n",
      "\n",
      "4. **Limitations in Exact Computation**: Despite their specialized design, LRMs have limitations in performing exact computations and may fail to consistently apply explicit algorithms across different types of puzzles.\n",
      "\n",
      "Overall, LRMs are distinguished from standard LLMs by their architectural enhancements and training objectives that focus on improving reasoning capabilities, although they still face challenges in certain areas of reasoning and computation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    client = get_qdrant_client()\n",
    "\n",
    "    if not is_qdrant_alive(client):\n",
    "        print(\"‚ùå Qdrant is not reachable. Exiting.\")\n",
    "        return\n",
    "\n",
    "    pdf_path = \"/Users/i589466/Desktop/Datenbanken/Datenbanken/the-illusion-of-thinking (1).pdf\"\n",
    "\n",
    "    print(\"üìÑ Loading and processing PDF...\")\n",
    "    chunks = load_pdf_and_chunk(pdf_path, chunk_size=500, overlap=50)\n",
    "    embeddings = embed_chunks(chunks)\n",
    "    store_embeddings_in_qdrant(client, \"db_Benny\", chunks, embeddings)\n",
    "\n",
    "    if is_qdrant_alive(client):\n",
    "        query= \"What distinguishes a Large Reasoning Model (LRM) from a standard Large Language Model (LLM) in terms of architecture and training objectives?\"\n",
    "        retrieved_chunks = retrieve_similar_chunks(query, client, \"db_Benny\", top_k=5)\n",
    "        answer = answer_with_context(query, retrieved_chunks)\n",
    "        print(f\"\\nüß† Answer from GPT-4o:\\n{answer}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
